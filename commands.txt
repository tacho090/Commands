# VIM

# set vim ruler
:set ruler

# See file metadata such as line and column number
g+ (Ctrl + g)

# LINUX

# kill processes on port 5000
kill $(lsof -t -i:5000)

# change directory ownership
sudo chown -R $USER:$USER /opt

# SHELL SCRIPTING

-d FILE True if file is a directory
-e FILE True if file exists
-f FILE True if file exists and is a regular file
-r FILE True if file is readable by you
-s FILE True if file exists and is not empty
-w FILE True if the file is writable by you
-x FILE True if the file is executable by you

-z STRING True if string is empty
-n STRING True if string is not empty
STRING1 = STRING2 True if the strings are equal
STRING1 != STRING2 True if the strings are not equal

# Arithmetic operators (tests)

arg1 -eq arg2 True if arg1 is equal to arg2
arg1 -ne arg2 True if arg1 is not equal to arg2

arg1 -lt arg2 True if arg1 is less than arg2
arg1 -le arg2 True if arg1 is less than or equal to arg2

arg1 -gt arg2 True if arg1 is greater than arg2
arg1 -ge arg2 True if arg1 is greater than or equal to arg2

# Making decisiones -  the if statemment

if [condition-is-true]
then
	command1
	command2
	command n
fi

# for loop

for VARIABLE_NAME in ITEM_1 ITEM_N
do
	command
done

for COLOR in red green blue
do
	echo "COLOR: $COLOR"
done

# store variables in list
COLORS="red green blue"

for COLOR in $COLORS
do
	echo "COLOR: $COLOR"
done

# positional parameters
$ script.sh parameter1 parameter2 parameter3



# MINIKUBE

# minikube version
minikube version

#start minikube with
# comma separated list of Kubernetes components to verify and wait for after starting a cluster
minikube start --wait=false

# enable addons for minikube
minikube addons enable dashboard

# get minikube ip
minikube ip

# NGINX

# start nginx
sudo systemctl start nginx

# stop nginx
sudo systemctl stop nginx

# restart nginx
sudo systemctl restart nginx

# reload nginx
sudo systemctl reload nginx

# show ip addresses
ip addr show | grep inet | awk '{ print $2; }' | sed 's/\/.*$//'

# nginx status
systemctl status nginx

# UFW

# allow nginx http
sudo ufw allow 'Nignx HTTP'

# ufw status
sudo ufw status

# app list ufw
sudo ufw app list

# YAML file conventions

# Each manifest needs four parts (root key:values in the file)
apiVersion:
kind:
metadata:
spec:

# JENKINS

# restart jenkins
sudo systemctl restart jenkins

# enable jenkins cli
java -jar jenkins-cli.jar -auth admin:08ce40a45bf14ba594650871aeb1054f -s http://localhost:8080/ -webSocket help

# KUBECTL

# port forwarding
# this means forward a port from your kubernetes cluster into a port inside your local machine
kubectl port-forward $(kubectl get pod --selector="app=do-kubernetes-sample-app" --output jsonpath='{.items[0].metadata.name}') 8080:80

# details of cluster
kubectl cluster-info

# get kubectl resources
kubectl api-resources

# explain services documentation
kubectl explain services.spec

# get api versions command
kubectl api-versions

# get all the keys each kind supports
kubectl explain services --recursive

# view the deployments in the cluster
kubectl get deployments

# create deployment
kubectl create deployment <deployment-name> --image=<image-name>

# get cluster pods
kubectl get pods

# expose deployment on specific port using NodePort
kubectl expose deployment <deployment-name> --port=<port-number> --type=NodePort

# expose service as a NodePort
kubectl expose deployment httpenv --port 8888 --name hhtpenv-np --type NodePort

# expose service as LoadBalancer
kubectl expose deployment httpenv --port 8888 --name httpenv-lb --type LoadBalancer

# run, create and expose generators dry run
kubectl create deployment sample --image nginx --dry-run -o yaml

# execute dry run on server
kubectl apply -f app.yml --server-dry-run

# difference between yml files which are about to be applied
kubectl diff -f app.yml

# deploy definition on katacoda
kubectl apply -f /opt/kubernetes-dashboard.yml

# sh into pod
kubectl exec --stdin --tty <pod-name> -- /bin/bash

# run deployment with properties
kubectl run http --image=katacoda/docker-http-server:latest --replicas=1

# describe deployment
kubectl describe deployment <deployment-name>

# curl url
curl <uri>:<port>
curl http://172.17.0.43:8000

# scale pods
kubectl scale --replicas=3 deployment <deployment-name>

# get logs for deployment, follow and show latest x amount of lines
kubectl logs deployment/my-apache --follow --tail 1

# combine logs from different pods
kubectl logs -l run=my-apache

# describe a single pod
kubectl describe pod my-apache-7b68fdd849-9js47

# live pods
kubectl get pods -w

# Scale replicaset definitions
kubectl replace -f replicaset-definition.yml

#Scale repplicaset by modifying parameter
kubectl scale --replicas=6 -f replicaset-definition.yml

# Scale replicasset by modifying parameter of replicaset definition
kubectl scale --replicas=6 replicaset myapp-replicaset

# Edit replicaset
kubectl edit replicaset new-replica-set

# a service is a stable address for pods
# if we want to connect to pods, we need a service
# coreDNS allows ud to resolve services by name
# service types: ClusterIP, NodePort, LoadBalancer, ExternalName
# ClusterIP (default)
  # Single, internal virtual IP allocated
  # Only reachable from within cluster (nodes and pods)
  # Pods can reach service on apps port number
  # Service always available on kubernetes
# NodePort
  # Designed for something outside the cluster to talk to your services
  # High port allocated on each node
  # Anyone can connect if they reach node
  # Port is open on every node's IP
  # Other pods need to be updated to this port
  # Service always available in Kubernetes
  # Create a NodePort creates a ClusterIP too
# LoadBalancer
  # Controls a LB endpoint external to te cluster
  # only available when infra provider gives you a LB
  # Creates NodePort+CLusterIP services, tells LB to send to NodePort
  # traffic coming in your cluster form external source
  # requires kubernetes provider
  # Creates a NodePort too
# ExternalName
  # Adds CNAME DNS record to CoreDNS only
  # Not used for Pods, but for giving pods a DNS name to use for something outside Kubernetes


# Volumes in Kubernetes

# Creating and connecting Volumes: 2 types
# Volumes
#  Tied to the lifecycle of a pod
#  All containers in a single pod can share them
# Persisten Volumes
#  Created at the cluster level, outlives a pod
#  Separates storage config from pod using it
#  multiple pods can share them
# CSI plugins are the new way to connect to storage

# Ingress

# None of our Service Types work at OSI Layer 7(HTTP)
# How do we route outside connections based on hostname or URL?
# Ingress controllers (optional) do this with 3rd party proxies
# nginx is popular, but Traefik, HAProxy, F5, Envoy, Istio

# DOCKER

# list all docker images
docker ps --all

# build docker file
docker build --tag <imagename>:<version> .

# run docker file
docker run --publish 8000:8080 --detach --name bb <imagename>:<version>

# delete docker image from list
docker rm --force <name>

# add jenkins to the docker group
sudo usermod -aG docker jenkins

# docker-compose up
docker-compose up -f docker-compose.yml

# docker-compose in detached mode to run services on the background
docker-compose up -d

# see what's currently running in docker-compose
docker-compose ps

# see environment variables of current docker-compose
docker-compose run <service-name> env

# stop current docker-compose
docker-compose stop

# bring everything down
docker-compose down --volumes

# docker update container
docker update <continer-name> --options

# docker build image
docker build -t av-app-image .

# delete all images
docker rmi -f $(docker images -aq)

# delete all containers including its columes use
docker rm -vf $(docker ps -aq)

# delete all running services
docker service rm $(docker service ls -q)

# docker service update
docker service update --image <image-name> <service to update> 

# update service port
d�ocker service update --publish-rm 8088 --publish-add 9090:80 web

# docker container list all that have been run
docker container ls -a

# Display the running process inside the container
docker container top <container-name>

# Inspect container
docker container inspect <container-name>

# run bash inside the container
docker container run -it --name proxy-x nginx bash

# run existing container interactively
docker container start -ai ubuntu

# pull docker image
docker pull <container-name>

# show docker container port
docker container port <container-name>

# list of all docker networks
docker network ls

# inspect docker network
docker network inspect <network-name>

# create virtual network
docker network create <network-name>

# connect docker container to a different network
docker network connect <network-name/id> <container-name/id>

# ping running container from another container
# you need to sh into container and install ping package first
docker container exec -it my_nginx ping new_nginx

# run container "elasticsearch"  on network "dude" with alias "search" 
docker container run -d --net dude --net-alias search elasticsearch:2

# review docker image layer history
docker history nginx:latest

# run docker container with health check
docker container run --name p2 -e POSTGRES_PASSWORD=password -d --health-cmd="pg_isready -U postgres || exit 1" postgres

# tag image 
docker tag hello-world 127.0.0.1:5000/hello-world

# build docker image with a specific dockerfile 	
docker build -f some-dockerfile

# prune images
docker image prune

# display volumes
docker volume ls

# run container with named volume
docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql-db:/var/lib/mysql mysql

# setup volumes/networks
docker-compose up

# stop all containers and remove /cont/vol/net
docker-compose down

# display docker-compose containers running
docker-compose ps

# see top processes on docker-compose
docker-compose top

# persistent data bind mounting
# binds local directory to directory inside dockerfile
# they both update simoultaneously
docker container run -d --name nginx -p 80:80 -v $(pwd):/usr/share/nginx/html nginx

# DOCKER REGISTRY

# run the registry image
docker container run -d -p 5000:5000 --name registry registry

# retag an existing image and push it to your new registry
docker tag hello-world 127.0.0.1:5000/hello-world
docker push 127.0.0.1:5000/hello-world

# remove that image form local cache and pull it from new registry
docker image remove hello-world
docker image remove 127.0.0.1:5000/hello-world
docker pull 127.0.0.1:5000/hello-world

# recreate a registry using a bind mount and see how it stores data
docker container run -d -p 5000:5000 --name registry -v $(pwd)/registry-data:/var/lib/registry registry

# DOCKER SWARM

# init docker swarm
docker swarm init

# show docker nodes
docker node ls

# docker service create
docker service create alpine ping 8.8.8.8

# information about the service
docker service ps <service-name>

# update number of replicas for docker service
docker service update <service-id/name> --replicas 3

# docker service update
docker service update --help

# create secret
docker secret create psql_user psql_user.txt

# DOCKER MACHINE

# Create node
docker-machine create <nodename>

# ssh into node
docker-machine ssh node1

# display environment variables for node1
docker-machine env node1

# program local machine to target node 1 inside virtual machine
eval $(docker-machine env node1)

# docker stack services
docker stack services voteapp

# DIGITAL OCEAN

# ssh into node on remote server
ssh root@<node-ip>

# install docker on remote server in one line
curl -fsSL https://get.docker.com -o get-docker.sh && sh get-docker.sh

# adviertise ip address from node inside digital ocean cluster
docker swarm init --advertise-addr 67.205.171.59

# update role of node from leader node
docker node update --role manager node2


#NATIVESCRIPT

# run nativescript application on emulator device
tns run android --device Pixel_3a_API_30_x86

# Verify available devices
tns device android --available-devices




# succesfully running an application on an android emulator
➜  HelloWorld git:(master) ✗ tns run android --device Pixel_3a_API_30_x86
Searching for devices...
Starting Android emulator with image Pixel_3a_API_30_x86
Waiting for emulator device initialization...
Your application will be deployed only on the device specified by the provided index or identifier.
Preparing project...
File change detected. Starting incremental webpack compilation...

webpack is watching the files…

Hash: 0f29a0178484f3e600ba
Version: webpack 4.44.2
Time: 41471ms
Built at: 10/30/2020 5:01:57 PM
              Asset      Size   Chunks             Chunk Names
          bundle.js  43.9 KiB   bundle  [emitted]  bundle
       package.json   1.5 KiB           [emitted]  
         runtime.js  77.1 KiB  runtime  [emitted]  runtime
tns-java-classes.js   0 bytes           [emitted]  
          vendor.js  16.9 MiB   vendor  [emitted]  vendor
Entrypoint bundle = runtime.js vendor.js bundle.js
[../$$_lazy_route_resource lazy recursive] ../$$_lazy_route_resource lazy namespace object 160 bytes {bundle} [built]
[./app.css] 2.03 KiB {bundle} [built]
[./app/app-routing.module.ts] 1.27 KiB {bundle} [built]
[./app/app.component.ts] 853 bytes {bundle} [built]
[./app/app.module.ts] 1.56 KiB {bundle} [built]
[./app/item/item-detail.component.ts] 2.02 KiB {bundle} [built]
[./app/item/item.service.ts] 1.74 KiB {bundle} [built]
[./app/item/items.component.ts] 1.91 KiB {bundle} [built]
[./main.ts] 2.3 KiB {bundle} [built]
[~/package.json] external "~/package.json" 42 bytes {bundle} [optional] [built]
    + 547 hidden modules
Webpack compilation complete. Watching for file changes.
Webpack build done!
Updating runtime package.json with configuration values...
Project successfully prepared (android)
Successfully transferred runtime.js on device emulator-5554.
Restarting application on device emulator-5554...
Successfully synced application org.nativescript.HelloWorld on device emulator-5554.
JS: HMR: Hot Module Replacement Enabled. Waiting for signal.
JS: Angular is running in development mode. Call enableProdMode() to enable production mode.
